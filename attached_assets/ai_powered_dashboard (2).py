# -*- coding: utf-8 -*-
"""AI_powered_dashboard

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1udfMJImLEc7G3kFrObL_bHC_NqQwB4md

#**Step 1: Install Dependencies and Import Libraries**
"""

# Install required packages
!pip install pandas numpy plotly scikit-learn fastapi uvicorn openai tiktoken dash dash-bootstrap-components jupyter-dash reportlab kaleido google-generativeai geohash2
# Import libraries
import pandas as pd
import numpy as np
import json
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import openai
import google.generativeai as genai
from google.colab import files
from dash import Dash, dcc, html, Input, Output, State
import dash_bootstrap_components as dbc
from scipy import stats
import warnings
import base64
from io import BytesIO
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet
import logging
import re
import geohash2
# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Ignore warnings
warnings.filterwarnings('ignore')

# Define the color palette
CHATEAU_GREEN = "#4bb051"
GRAY_NURSE = "#dee8dd"
MINE_SHAFT = "#323232"
MOSS_GREEN = "#9cd4a4"

# Placeholder for Oizom logo
logo_path = "oizom_logo.png"  # Replace with actual path

"""#**Step 2: Data Upload and Preprocessing**"""

"""#**Step 2: Data Upload and Preprocessing**"""

# Function to upload files
def upload_files():
    """Prompts the user to upload multiple files and returns a list of file names."""
    uploaded = files.upload()
    file_names = list(uploaded.keys())
    if not file_names:
        raise ValueError("No files uploaded. Please upload at least one data file.")
    logger.info(f"✅ {len(file_names)} file(s) uploaded successfully!")
    return file_names

# Function to load, clean data, and generate geohash
def load_and_clean_data(file_paths):
    """Loads multiple CSV/Excel files, cleans them, merges them into a single DataFrame, and adds a Geohash column."""
    all_data = []
    all_columns = set()

    # Define device coordinates mapping (latitude, longitude)
    device_coordinates = {
        523005: (23.02909505, 72.49078965),
        523011: (23.0309101, 72.5088321),
        523047: (23.0692036, 72.5653925),
        523082: (23.0850114, 72.5751516),
        523093: (23.096915, 72.527362),
        524037: (23.04836488, 72.68863108),
        524046: (23.0428964, 72.4749039),
        524049: (23.0777287, 72.5056656),
        524062: (23.12348122, 72.53853052),
        524089: (23.02815923, 72.50001528),
        524091: (23.0087287, 72.4551301),
    }
    device_geohash = {device_id: geohash2.encode(lat, lon, precision=7) for device_id, (lat, lon) in device_coordinates.items()}

    for file_path in file_paths:
        try:
            # Load file based on extension
            file_extension = file_path.split('.')[-1].lower()
            if file_extension == 'csv':
                df = pd.read_csv(file_path, encoding='utf-8-sig', delimiter=',', dtype={'To Date': str})
            elif file_extension in ['xls', 'xlsx']:
                df = pd.read_excel(file_path, dtype={'To Date': str})
            else:
                logger.warning(f"⚠️ Unsupported file format: {file_path}")
                continue

            df.columns = df.columns.str.strip()
            logger.info(f"Loaded columns for {file_path}: {list(df.columns)}")

            # Datetime conversion
            if "To Date" in df.columns:
                df.rename(columns={"To Date": "Datetime"}, inplace=True)
                df["Datetime"] = pd.to_datetime(df["Datetime"], format="%d-%m-%Y %H:%M", errors="coerce", dayfirst=True)
                if df["Datetime"].isna().any():
                    logger.warning(f"⚠️ Some Datetime values invalid in {file_path}; attempting fallback parsing.")
                    df["Datetime"] = pd.to_datetime(df["Datetime"], dayfirst=True, errors="coerce")
                df["Time"] = df["Datetime"].dt.strftime('%H:%M:%S')
                if df["Datetime"].isna().all():
                    logger.error(f"❌ All Datetime values are NaT in {file_path}. Skipping.")
                    continue
                logger.info(f"✅ Datetime column converted successfully in {file_path}")
            else:
                logger.error(f"❌ 'To Date' column not found in {file_path}. Skipping.")
                continue

            # Rename columns
            column_mapping = {
                'PM₂.₅ (µg/m³)': 'PM2.5', 'PM₁₀ (µg/m³)': 'PM10', 'PM₁ (µg/m³ )': 'PM1',
                'PM₁₀₀ (µg/m³ )': 'PM100', 'R. Humidity (%)': 'Humidity',
                'Temperature (°C)': 'Temperature', 'wind direction (degree)': 'Wind_Direction',
                'wind speed (m/s)': 'Wind_Speed'
            }
            df.rename(columns=column_mapping, inplace=True)
            all_columns.update(df.columns)
            logger.info(f"Columns after renaming in {file_path}: {list(df.columns)}")

            # Extract Device_ID from filename (e.g., AQ0523005.csv -> 523005)
            device_id_match = re.search(r'AQ0(\d{6})', file_path)
            if not device_id_match:
                logger.error(f"❌ Could not extract Device_ID from {file_path}. Skipping.")
                continue
            device_id = int(device_id_match.group(1))  # Extract the 6-digit number after 'AQ0'
            if device_id not in device_coordinates:
                logger.warning(f"⚠️ Device_ID {device_id} from {file_path} not in coordinates mapping. Geohash will be NaN.")
            df['Device_ID'] = device_id
            df['Geohash'] = df['Device_ID'].map(device_geohash)
            logger.info(f"✅ Geohash assigned in {file_path}: {df['Geohash'].iloc[0] if not df['Geohash'].isna().all() else 'NaN'}")

            # Convert objects to numeric
            df.replace('-', pd.NA, inplace=True)
            for col in df.select_dtypes(include=['object']).columns:
                if col not in ['Time', 'Device_ID', 'Geohash']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            # Validate key columns
            if 'PM2.5' in df.columns:
                invalid_pm25 = df[(df['PM2.5'] < 0) | (df['PM2.5'] > 500)].index
                df.loc[invalid_pm25, 'PM2.5'] = pd.NA
                if not invalid_pm25.empty:
                    logger.warning(f"⚠️ {len(invalid_pm25)} rows with invalid PM2.5 values in {file_path}; set to NaN.")
            if 'Wind_Direction' in df.columns:
                invalid_wind = df[(df['Wind_Direction'] < 0) | (df['Wind_Direction'] > 360)].index
                df.loc[invalid_wind, 'Wind_Direction'] = pd.NA
                if not invalid_wind.empty:
                    logger.warning(f"⚠️ {len(invalid_wind)} rows with invalid Wind_Direction values in {file_path}; set to NaN.")

            # Fill missing values for continuous variables
            for col in ['Temperature', 'Humidity', 'Wind_Speed']:
                if col in df.columns:
                    df[col] = df[col].ffill().bfill()

            # Fill calibration factors
            cf_columns = ['P1_CF', 'P2_CF', 'P3_CF', 'P4_CF', 'HUM_CF', 'TEMP_CF']
            for col in cf_columns:
                if col in df.columns:
                    df[col] = df[col].fillna(100.0)

            all_data.append(df)
        except Exception as e:
            logger.error(f"❌ Error processing {file_path}: {e}")
            continue

    if all_data:
        # Ensure consistent columns across all DataFrames
        for i in range(len(all_data)):
            missing_cols = all_columns - set(all_data[i].columns)
            for col in missing_cols:
                all_data[i][col] = pd.NA
        merged_df = pd.concat(all_data, ignore_index=True)
        merged_df.sort_values(by=['Device_ID', 'Datetime'], ascending=[True, True], inplace=True)
        logger.info("✅ Data merged successfully.")
        return merged_df
    logger.error("❌ No valid data processed.")
    return None

# Execute file upload and data preprocessing
try:
    file_paths = upload_files()
    df = load_and_clean_data(file_paths)
    if df is not None:
        logger.info("\n✅ Merged Data Preview:", extra={'flush': True})
        display_columns = ['Datetime', 'FAN', 'Humidity', 'PM2.5', 'PM10', 'PM1', 'PM100', 'S1',
                          'Temperature', 'Wind_Direction', 'Wind_Speed', 'P1_CF', 'P2_CF', 'P3_CF',
                          'P4_CF', 'HUM_CF', 'TEMP_CF', 'Time', 'Device_ID', 'Geohash']
        display_columns = [col for col in display_columns if col in df.columns]
        display_df = df[display_columns]
        logger.info("Preview as string:\n" + display_df.head().to_string(), extra={'flush': True})
        display(display_df.head())
        # Save the file and log success
        df.to_csv("merged_data.csv", index=False)
        logger.info("\n✅ Merged data saved as 'merged_data.csv'.", extra={'flush': True})
    else:
        logger.error("❌ No data to display. Check uploaded files.", extra={'flush': True})
except Exception as e:
    logger.error(f"❌ Execution failed: {e}", extra={'flush': True})

"""#**Step 3: AI-Based Recommendations**"""

# Set your API keys
gemini_api_key = "AIzaSyAeRYT_M-gbsPopT42wygWBUDx9pCFsdWI"  # Replace with your Gemini API key
genai.configure(api_key=gemini_api_key)

# Load the preprocessed data
try:
    df = pd.read_csv("merged_data.csv")
    logger.info("✅ Loaded merged_data.csv successfully.", extra={'flush': True})
except FileNotFoundError:
    logger.error("❌ merged_data.csv not found. Ensure Step 2 completed successfully.", extra={'flush': True})
    raise
except Exception as e:
    logger.error(f"❌ Error loading merged_data.csv: {e}", extra={'flush': True})
    raise

# Function to get AI-based recommendations using Gemini
def get_ai_recommendations(df):
    summary = f"""
    Dataset Summary:
    - Number of rows: {len(df)}
    - Number of columns: {len(df.columns)}
    - Columns and data types: {df.dtypes.to_dict()}
    - Basic statistics: {df.describe().to_dict()}
    - Sample data (first 5 rows): {df.head().to_dict()}
    - Unique Device IDs: {df['Device_ID'].unique().tolist()}
    - Unique Geohashes: {df['Geohash'].unique().tolist() if 'Geohash' in df.columns else 'Not available'}

    The dataset contains environmental sensor data with the following columns:
    - Datetime: Timestamp of the measurement
    - FAN: Fan status (binary)
    - Humidity: Relative humidity (%)
    - PM2.5: Particulate matter 2.5 (µg/m³)
    - PM10: Particulate matter 10 (µg/m³)
    - PM1: Particulate matter 1 (µg/m³)
    - PM100: Particulate matter 100 (µg/m³)
    - S1: Sensor 1 reading
    - SG: Sensor group reading
    - Temperature: Temperature (°C)
    - Wind_Direction: Wind direction (degrees)
    - Wind_Speed: Wind speed (m/s)
    - P1_CF, P2_CF, P3_CF, P4_CF, HUM_CF, TEMP_CF: Calibration factors
    - Time: Time of day (HH:MM:SS)
    - Device_ID: Device identifier
    - Geohash: Geospatial hash (precision 7, approx. 153m resolution)

    Please analyze the data and suggest:
    1. Potential relationships between variables (e.g., correlations, trends).
    2. Recommended graph types for visualization in a dashboard.
    """
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(summary)
        recommendations = response.text
        return recommendations
    except Exception as e:
        return f"❌ Error getting Gemini API recommendations: {e}"

# Get AI recommendations

recommendations = get_ai_recommendations(df)
logger.info("\n🎯 Gemini AI-Based Recommendations for Relationships & Graph Selection:", extra={'flush': True})
logger.info(recommendations, extra={'flush': True})

# Basic correlation analysis to validate AI suggestions

logger.info("\n🔍 Enhanced Correlation Matrix (Key Variables):", extra={'flush': True})
key_columns = ['PM2.5', 'PM10', 'PM1', 'Temperature', 'Humidity', 'Wind_Speed', 'Wind_Direction']
available_columns = [col for col in key_columns if col in df.columns]
df_clean = df[available_columns].fillna(df[available_columns].mean())  # Fill NaN with mean instead of dropping
correlation_matrix = df_clean.corr()
logger.info(correlation_matrix.to_string(), extra={'flush': True})

# Plot correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title("Correlation Matrix of Key Environmental Variables")
plt.show()

"""#**Step 4: Initialize Dash Application and Load Data**"""

# Initialize Dash app
app = Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])

# Load the preprocessed data for the dashboard
try:
    df = pd.read_csv("merged_data.csv")
    df['Datetime'] = pd.to_datetime(df['Datetime'])
    df['Hour'] = df['Datetime'].dt.hour
    logger.info(f"Data loaded successfully. Columns: {df.columns.tolist()}", extra={'flush': True})
except FileNotFoundError:
    logger.error("❌ merged_data.csv not found. Ensure Step 2 completed successfully.", extra={'flush': True})
    raise
except Exception as e:
    logger.error(f"❌ Error loading data: {e}", extra={'flush': True})
    raise

# Gemini's Recommendations (hard-coded for now, extended with Geohash-based visualization)
gemini_recommendations = """
Recommended Graph Types for Visualization in a Dashboard:
* **Overview Panel:**
    * **Gauge Charts:** For key metrics like current PM2.5, PM10, Temperature, and Humidity.
    * **Line Graphs:** Trends for PM2.5, PM10, and Temperature over the last 24 hours.
    * **Bar Charts:** Compare average PM2.5, Temperature across devices.
* **Detailed Analysis Panels:**
    * **Scatter Plots:** Relationships between Temperature and Humidity, Wind Speed and PM2.5.
    * **Line Graphs:** Diurnal patterns of PM2.5, Temperature, and Humidity.
    * **Histograms:** PM2.5, Temperature, Humidity distributions.
* **Geospatial Analysis Panel:**
    * **Scatter Map:** Plot PM2.5 levels by Geohash to show spatial distribution.
* **Data Quality Panel:**
    * **Bar Charts:** Missing data percentage per variable.
"""

# Parse Gemini's recommendations into configurations
def parse_gemini_recommendations(text):
    configurations = []
    lines = text.split('\n')
    current_section = None
    for line in lines:
        line = line.strip()
        if 'Overview Panel' in line:
            current_section = 'overview'
        elif 'Detailed Analysis Panels' in line:
            current_section = 'analysis'
        elif 'Geospatial Analysis Panel' in line:
            current_section = 'geospatial'
        elif 'Data Quality Panel' in line:
            current_section = 'data_quality'
        if current_section == 'overview':
            if 'Gauge Charts' in line:
                vars_list = re.findall(r'PM2\.5|PM10|Temperature|Humidity', line)
                for var in vars_list:
                    configurations.append({'vars': (var,), 'graph_type': 'gauge'})
            elif 'Line Graphs' in line:
                vars_list = re.findall(r'PM2\.5|PM10|Temperature|Humidity', line)
                for var in vars_list:
                    configurations.append({'vars': ('Datetime', var), 'graph_type': 'line'})
            elif 'Bar Charts' in line:
                vars_list = re.findall(r'PM2\.5|Temperature', line)
                for var in vars_list:
                    configurations.append({'vars': (var,), 'graph_type': 'bar'})
        elif current_section == 'analysis':
            if 'Scatter Plots' in line:
                pairs = re.findall(r'(\w+\s*(?:and|vs\.)\s*\w+)', line)
                for pair in pairs:
                    if 'and' in pair:
                        vars_pair = [v.strip() for v in pair.split('and')]
                        configurations.append({'vars': (vars_pair[0], vars_pair[1]), 'graph_type': 'scatter'})
            elif 'Line Graphs' in line:
                vars_list = re.findall(r'PM2\.5|Temperature|Humidity', line)
                for var in vars_list:
                    configurations.append({'vars': ('Hour', var), 'graph_type': 'line'})
            elif 'Histograms' in line:
                vars_list = re.findall(r'PM2\.5|Temperature|Humidity', line)
                for var in vars_list:
                    configurations.append({'vars': (var,), 'graph_type': 'histogram'})
        elif current_section == 'geospatial':
            if 'Scatter Map' in line:
                vars_list = re.findall(r'PM2\.5|PM10|Temperature|Humidity', line)
                for var in vars_list:
                    configurations.append({'vars': (var, 'Geohash'), 'graph_type': 'scatter_map'})
        elif current_section == 'data_quality':
            if 'Bar Charts' in line:
                configurations.append({'vars': None, 'graph_type': 'bar_missing'})
    return configurations

visualization_configs = parse_gemini_recommendations(gemini_recommendations)
logger.info(f"Visualization configs: {visualization_configs}", extra={'flush': True})

"""# **Step 5: Define Helper Functions for Insights and KPIs**"""

# Function to generate dynamic insights
def generate_dynamic_insights(config, data, selected_devices):
    try:
        # Filter data by selected devices
        if selected_devices:
            data = data[data['Device_ID'].isin(selected_devices)]
        if data.empty:
            return [f"No data available for selected devices: {selected_devices}"], "No data available."

        vars_tuple = config['vars']
        graph_type = config['graph_type']
        insights = []
        summary = ""

        if graph_type == 'gauge':
            var = vars_tuple[0]
            if var not in data.columns or data[var].isna().all():
                insights.append(f"Data for {var} not available.")
                summary = f"No data for {var}."
            else:
                plot_data = data.groupby('Device_ID')[var].last()
                mean_val = plot_data.mean()
                max_val = plot_data.max()
                max_dev = plot_data.idxmax()
                insights.append(f"Average {var} across devices: {mean_val:.2f}")
                insights.append(f"Maximum {var} recorded: {max_val:.2f} by Device {max_dev}")
                if var == 'PM2.5' and mean_val > 15:
                    insights.append(f"Warning: Average PM2.5 exceeds WHO guideline of 15 µg/m³ by {(mean_val-15)/15*100:.2f}%")
                summary = f"Current {var} average: {mean_val:.2f}, Max: {max_val:.2f} (Device {max_dev})"

        elif graph_type == 'line':
            x_var, y_var = vars_tuple
            if x_var not in data.columns or y_var not in data.columns or data[y_var].isna().all():
                insights.append(f"Data for {x_var} or {y_var} not available.")
                summary = f"No data for {x_var} or {y_var}."
            else:
                last_24h = data[data['Datetime'] >= data['Datetime'].max() - pd.Timedelta(hours=24)]
                if last_24h.empty or last_24h[y_var].isna().all():
                    insights.append(f"No data for {y_var} in last 24 hours.")
                    summary = f"No data for {y_var}."
                else:
                    plot_data = last_24h.groupby(['Device_ID', x_var])[y_var].mean().reset_index()
                    max_val = plot_data[y_var].max()
                    min_val = plot_data[y_var].min()
                    max_dev = plot_data.loc[plot_data[y_var].idxmax(), 'Device_ID']
                    min_dev = plot_data.loc[plot_data[y_var].idxmin(), 'Device_ID']
                    insights.append(f"Maximum {y_var} in last 24 hours: {max_val:.2f} (Device {max_dev})")
                    insights.append(f"Minimum {y_var} in last 24 hours: {min_val:.2f} (Device {min_dev})")
                    if len(selected_devices) > 1:
                        dev_means = plot_data.groupby('Device_ID')[y_var].mean()
                        highest_dev = dev_means.idxmax()
                        lowest_dev = dev_means.idxmin()
                        insights.append(f"Device {highest_dev} has the highest average {y_var} ({dev_means[highest_dev]:.2f}), while Device {lowest_dev} has the lowest ({dev_means[lowest_dev]:.2f})")
                    summary = f"Max {y_var}: {max_val:.2f} (Device {max_dev}), Min: {min_val:.2f} (Device {min_dev})"

        elif graph_type == 'scatter':
            x_var, y_var = vars_tuple
            if x_var not in data.columns or y_var not in data.columns or data[x_var].isna().all() or data[y_var].isna().all():
                insights.append(f"Data for {x_var} or {y_var} not available.")
                summary = f"No data for {x_var} or {y_var}."
            else:
                correlation = data[x_var].corr(data[y_var])
                insights.append(f"Correlation between {x_var} and {y_var}: {correlation:.2f}")
                if abs(correlation) > 0.7:
                    insights.append(f"Strong {'positive' if correlation > 0 else 'negative'} relationship detected between {x_var} and {y_var}.")
                elif abs(correlation) < 0.3:
                    insights.append(f"Weak relationship between {x_var} and {y_var}.")
                summary = f"Correlation: {correlation:.2f}"

        elif graph_type == 'histogram':
            var = vars_tuple[0]
            if var not in data.columns or data[var].isna().all():
                insights.append(f"Data for {var} not available.")
                summary = f"No data for {var}."
            else:
                mean_val = data[var].mean()
                std_val = data[var].std()
                insights.append(f"Average {var}: {mean_val:.2f}")
                insights.append(f"Standard Deviation of {var}: {std_val:.2f}")
                if len(selected_devices) > 1:
                    dev_means = data.groupby('Device_ID')[var].mean()
                    highest_dev = dev_means.idxmax()
                    lowest_dev = dev_means.idxmin()
                    insights.append(f"Device {highest_dev} has the highest average {var} ({dev_means[highest_dev]:.2f}), while Device {lowest_dev} has the lowest ({dev_means[lowest_dev]:.2f})")
                summary = f"Average {var}: {mean_val:.2f}, Std: {std_val:.2f}"

        elif graph_type == 'bar':
            var = vars_tuple[0]
            if var not in data.columns or data[var].isna().all():
                insights.append(f"Data for {var} not available.")
                summary = f"No data for {var}."
            else:
                plot_data = data.groupby('Device_ID')[var].mean().reset_index()
                max_dev = plot_data[var].idxmax()
                max_dev_id = plot_data.loc[max_dev, 'Device_ID']
                min_dev = plot_data[var].idxmin()
                min_dev_id = plot_data.loc[min_dev, 'Device_ID']
                insights.append(f"Device {max_dev_id} has the highest average {var}: {plot_data[var].max():.2f}")
                insights.append(f"Device {min_dev_id} has the lowest average {var}: {plot_data[var].min():.2f}")
                summary = f"Highest {var} in {max_dev_id}: {plot_data[var].max():.2f}, Lowest in {min_dev_id}: {plot_data[var].min():.2f}"

        elif graph_type == 'bar_missing':
            missing_data = data.isna().mean() * 100
            if missing_data.empty:
                insights.append("No missing data to analyze.")
                summary = "No missing data."
            else:
                max_var = missing_data.idxmax()
                max_missing = missing_data.max()
                min_var = missing_data.idxmin()
                min_missing = missing_data.min()
                insights.append(f"Highest missing data in {max_var}: {max_missing:.2f}%")
                insights.append(f"Lowest missing data in {min_var}: {min_missing:.2f}%")
                summary = f"Max missing in {max_var}: {max_missing:.2f}%, Min missing in {min_var}: {min_missing:.2f}%"

        elif graph_type == 'scatter_map':
            var, geohash_var = vars_tuple
            if var not in data.columns or geohash_var not in data.columns or data[var].isna().all() or data[geohash_var].isna().all():
                insights.append(f"Data for {var} or {geohash_var} not available.")
                summary = f"No data for {var} or {geohash_var}."
            else:
                # Group by Geohash to find spatial patterns
                plot_data = data.groupby(geohash_var)[var].mean().reset_index()
                max_loc = plot_data[var].idxmax()
                max_loc_geohash = plot_data.loc[max_loc, geohash_var]
                max_val = plot_data[var].max()
                min_loc = plot_data[var].idxmin()
                min_loc_geohash = plot_data.loc[min_loc, geohash_var]
                min_val = plot_data[var].min()
                insights.append(f"Highest average {var} at location {max_loc_geohash}: {max_val:.2f}")
                insights.append(f"Lowest average {var} at location {min_loc_geohash}: {min_val:.2f}")
                unique_locations = data[geohash_var].nunique()
                insights.append(f"Data spans {unique_locations} unique locations.")
                summary = f"Highest {var} at {max_loc_geohash}: {max_val:.2f}, Lowest at {min_loc_geohash}: {min_val:.2f}"

        return insights, summary
    except Exception as e:
        logger.error(f"Error generating insights: {e}", extra={'flush': True})
        return [f"Error generating insights: {e}"], "Error generating summary"

# Function to compute KPIs
def compute_kpis(data):
    try:
        kpis = {}
        # Existing KPIs
        pm25_values = data['PM2.5'].dropna() if 'PM2.5' in data.columns else pd.Series([])
        kpis['Average PM2.5 Level'] = pm25_values.mean() if not pm25_values.empty else 0
        exceedances = len(pm25_values[pm25_values > 35]) if not pm25_values.empty else 0
        total = len(pm25_values) if not pm25_values.empty else 1
        kpis['Exceedance Rate (PM2.5 > 35 µg/m³)'] = (exceedances / total * 100)
        temp_values = data['Temperature'].dropna() if 'Temperature' in data.columns else pd.Series([])
        kpis['Temperature Range'] = (temp_values.max() - temp_values.min()) if not temp_values.empty else 0
        wind_values = data['Wind_Speed'].dropna() if 'Wind_Speed' in data.columns else pd.Series([])
        kpis['Average Wind Speed'] = wind_values.mean() if not wind_values.empty else 0

        # New Geohash-based KPIs
        if 'Geohash' in data.columns:
            kpis['Number of Unique Locations'] = data['Geohash'].nunique()
            if 'PM2.5' in data.columns and not data['PM2.5'].isna().all():
                worst_location = data.groupby('Geohash')['PM2.5'].mean().idxmax()
                worst_location_value = data.groupby('Geohash')['PM2.5'].mean().max()
                kpis['Worst Air Quality Location'] = f"{worst_location} (PM2.5: {worst_location_value:.2f})"
            else:
                kpis['Worst Air Quality Location'] = "Not available (PM2.5 data missing)"

        logger.info(f"KPIs computed: {kpis}", extra={'flush': True})
        return kpis
    except Exception as e:
        logger.error(f"Error computing KPIs: {e}", extra={'flush': True})
        return {
            'Average PM2.5 Level': 0,
            'Exceedance Rate (PM2.5 > 35 µg/m³)': 0,
            'Temperature Range': 0,
            'Average Wind Speed': 0,
            'Number of Unique Locations': 0,
            'Worst Air Quality Location': 'Error'
        }

"""#**Step 6: Generate PDF Report**"""

"""#**Step 6: Generate PDF Report**"""

# Function to generate PDF report
def generate_pdf_report(overview_fig, analysis_fig, data_quality_fig, geospatial_fig, insights_dict, kpis, data, logo_path=None):
    try:
        logger.info("Starting PDF generation", extra={'flush': True})
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=letter)
        styles = getSampleStyleSheet()
        elements = []

        # Header with optional logo
        logger.debug("Adding header", extra={'flush': True})
        if logo_path:
            try:
                with open(logo_path, 'rb') as img_file:
                    img_data = base64.b64encode(img_file.read()).decode('utf-8')
                logo = Image(BytesIO(base64.b64decode(img_data)), width=100, height=50)
                elements.append(logo)
            except FileNotFoundError as e:
                logger.warning(f"Logo file not found: {e}", extra={'flush': True})
                elements.append(Paragraph("Oizom Air Quality Report (Logo Missing)", styles['Title']))
        else:
            elements.append(Paragraph("Oizom Air Quality Report", styles['Title']))
        elements.append(Spacer(1, 12))
        elements.append(Paragraph(f"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Normal']))
        elements.append(Spacer(1, 12))

        # Executive Summary
        logger.debug("Adding executive summary", extra={'flush': True})
        elements.append(Paragraph("Executive Summary", styles['Heading1']))
        pm25_avg = kpis.get('Average PM2.5 Level', 0)
        summary_text = f"This report provides an analysis of air quality data for the selected devices. The average PM2.5 level is {pm25_avg:.2f} µg/m³, "
        if pm25_avg > 15:
            summary_text += "which exceeds the WHO guideline of 15 µg/m³, indicating potential health risks."
        else:
            summary_text += "which is within the WHO guideline of 15 µg/m³, indicating acceptable air quality."
        elements.append(Paragraph(summary_text, styles['Normal']))
        elements.append(Spacer(1, 12))

        # Key Performance Indicators
        logger.debug("Adding KPIs", extra={'flush': True})
        elements.append(Paragraph("Key Performance Indicators", styles['Heading2']))
        elements.append(Spacer(1, 6))
        kpi_table_data = [["Metric", "Value"]]
        for title, value in kpis.items():
            if isinstance(value, (int, float)):
                kpi_table_data.append([title, f"{value:.2f}" + ("%" if "Rate" in title else "")])
            else:
                kpi_table_data.append([title, str(value)])  # Handle string values (e.g., Worst Air Quality Location)
        table = Table(kpi_table_data)
        table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 14),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.lightgrey),
            ('TEXTCOLOR', (0, 1), (-1, -1), colors.black),
            ('FONTSIZE', (0, 1), (-1, -1), 12),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        elements.append(table)
        elements.append(Spacer(1, 12))

        # Graphs and Insights
        logger.debug("Adding graphs and insights", extra={'flush': True})
        elements.append(Paragraph("Graphs and Insights", styles['Heading2']))
        elements.append(Spacer(1, 12))

        # Overview Section
        try:
            if overview_fig:
                logger.debug("Adding overview graph", extra={'flush': True})
                img_buffer = BytesIO()
                overview_fig.write_image(img_buffer, format='png')
                img_buffer.seek(0)
                img = Image(img_buffer, width=400, height=300)
                elements.append(Paragraph("Overview: Trends Over Time", styles['Heading3']))
                elements.append(img)
                elements.append(Spacer(1, 12))
            else:
                logger.warning("Overview figure is None. Ensure data is available for trends.", extra={'flush': True})
                elements.append(Paragraph("Overview Graph: Not Available (No Data)", styles['Heading3']))
        except Exception as e:
            logger.error(f"Error adding overview graph: {e}", extra={'flush': True})
            elements.append(Paragraph(f"Overview Graph: Error - {str(e)}. Install 'kaleido' with 'pip install -U kaleido' if not already installed.", styles['Heading3']))
        for insight in insights_dict.get('overview', []):
            elements.append(Paragraph(insight, styles['Normal']))
        elements.append(Spacer(1, 12))

        # Analysis Section
        try:
            if analysis_fig:
                logger.debug("Adding analysis graph", extra={'flush': True})
                img_buffer = BytesIO()
                analysis_fig.write_image(img_buffer, format='png')
                img_buffer.seek(0)
                img = Image(img_buffer, width=400, height=300)
                elements.append(Paragraph("Detailed Analysis: Correlation", styles['Heading3']))
                elements.append(img)
                elements.append(Spacer(1, 12))
            else:
                logger.warning("Analysis figure is None. Ensure data is available for correlations.", extra={'flush': True})
                elements.append(Paragraph("Analysis Graph: Not Available (No Data)", styles['Heading3']))
        except Exception as e:
            logger.error(f"Error adding analysis graph: {e}", extra={'flush': True})
            elements.append(Paragraph(f"Analysis Graph: Error - {str(e)}. Install 'kaleido' with 'pip install -U kaleido' if not already installed.", styles['Heading3']))
        for insight in insights_dict.get('analysis', []):
            elements.append(Paragraph(insight, styles['Normal']))
        elements.append(Spacer(1, 12))

        # Geospatial Analysis Section
        try:
            if geospatial_fig:
                logger.debug("Adding geospatial graph", extra={'flush': True})
                img_buffer = BytesIO()
                geospatial_fig.write_image(img_buffer, format='png')
                img_buffer.seek(0)
                img = Image(img_buffer, width=400, height=300)
                elements.append(Paragraph("Geospatial Analysis: PM2.5 by Location", styles['Heading3']))
                elements.append(img)
                elements.append(Spacer(1, 12))
            else:
                logger.warning("Geospatial figure is None. Ensure Geohash and PM2.5 data are available.", extra={'flush': True})
                elements.append(Paragraph("Geospatial Graph: Not Available (No Data)", styles['Heading3']))
        except Exception as e:
            logger.error(f"Error adding geospatial graph: {e}", extra={'flush': True})
            elements.append(Paragraph(f"Geospatial Graph: Error - {str(e)}. Install 'kaleido' with 'pip install -U kaleido' if not already installed.", styles['Heading3']))
        for insight in insights_dict.get('geospatial', []):
            elements.append(Paragraph(insight, styles['Normal']))
        elements.append(Spacer(1, 12))

        # Data Quality Section
        try:
            if data_quality_fig:
                logger.debug("Adding data quality graph", extra={'flush': True})
                img_buffer = BytesIO()
                data_quality_fig.write_image(img_buffer, format='png')
                img_buffer.seek(0)
                img = Image(img_buffer, width=400, height=300)
                elements.append(Paragraph("Data Quality: Missing Data", styles['Heading3']))
                elements.append(img)
                elements.append(Spacer(1, 12))
            else:
                logger.warning("Data Quality figure is None. Ensure data is available for quality analysis.", extra={'flush': True})
                elements.append(Paragraph("Data Quality Graph: Not Available (No Data)", styles['Heading3']))
        except Exception as e:
            logger.error(f"Error adding data quality graph: {e}", extra={'flush': True})
            elements.append(Paragraph(f"Data Quality Graph: Error - {str(e)}. Install 'kaleido' with 'pip install -U kaleido' if not already installed.", styles['Heading3']))
        for insight in insights_dict.get('data_quality', []):
            elements.append(Paragraph(insight, styles['Normal']))
        elements.append(Spacer(1, 12))

        # Conclusion and Recommendations
        logger.debug("Adding conclusion and recommendations", extra={'flush': True})
        elements.append(Paragraph("Conclusion and Recommendations", styles['Heading2']))
        conclusion = "Based on the analysis, the following recommendations are suggested:\n"
        if pm25_avg > 15:
            conclusion += "- Increase ventilation or deploy air purifiers in areas with high PM2.5 levels.\n"
            if len(data['Device_ID'].unique()) > 1:
                max_pm25_dev = data.groupby('Device_ID')['PM2.5'].mean().idxmax()
                conclusion += f"- Prioritize Device {max_pm25_dev} for immediate action due to the highest PM2.5 average.\n"
            if 'Worst Air Quality Location' in kpis and 'Not available' not in kpis['Worst Air Quality Location']:
                conclusion += f"- Focus on the worst-affected location: {kpis['Worst Air Quality Location']}.\n"
        if kpis.get('Exceedance Rate (PM2.5 > 35 µg/m³)', 0) > 50:
            conclusion += "- Investigate sources of high PM2.5 emissions, such as traffic or industrial activity.\n"
        conclusion += "- Continue monitoring air quality to identify trends and potential risks."
        elements.append(Paragraph(conclusion, styles['Normal']))

        # Build the PDF
        logger.debug("Building PDF", extra={'flush': True})
        doc.build(elements)
        pdf = buffer.getvalue()
        logger.info(f"PDF generated successfully, size: {len(pdf)} bytes", extra={'flush': True})
        buffer.close()
        return pdf

    except Exception as e:
        logger.error(f"Error generating PDF: {e}", extra={'flush': True})
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=letter)
        styles = getSampleStyleSheet()
        elements = [
            Paragraph("Error Generating Report", styles['Title']),
            Paragraph(f"An error occurred: {str(e)}", styles['Normal'])
        ]
        doc.build(elements)
        pdf = buffer.getvalue()
        buffer.close()
        return pdf

"""#**Step 7: Define Dashboard Layout**"""

# Sidebar
sidebar = html.Div([
    html.H3("Air Quality Dashboard", style={'color': GRAY_NURSE, 'padding': '20px'}),
    html.Hr(),
    # Beginner Mode Toggle
    html.Div([
        dbc.Button("Beginner Mode", id="mode-toggle", color="success", style={'margin': '10px'}),
    ], style={'textAlign': 'center'}),
    dcc.Dropdown(id='device-dropdown', options=[{'label': d, 'value': d} for d in df['Device_ID'].unique()],
                 value=[df['Device_ID'].unique()[0]], multi=True, placeholder="Select Device(s)",
                 style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
    dcc.Dropdown(id='time-range', options=[
        {'label': 'Last 24 Hours', 'value': '24h'},
        {'label': 'Last 48 Hours', 'value': '48h'},
        {'label': 'Last 7 Days', 'value': '7d'},
        {'label': 'All Time', 'value': 'all'}
    ], value='24h', placeholder="Select Time Range",
                 style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
    dcc.DatePickerRange(id='date-picker', min_date_allowed=df['Datetime'].min(),
                        max_date_allowed=df['Datetime'].max(), start_date=df['Datetime'].min(),
                        end_date=df['Datetime'].max(), style={'margin': '10px'}),
    html.Hr(),
    html.P("Navigation", style={'color': GRAY_NURSE, 'padding': '10px'}),
    dbc.Nav([
        dbc.NavLink("Overview", href="#overview", style={'color': MOSS_GREEN}),
        dbc.NavLink("Detailed Analysis", href="#analysis", style={'color': MOSS_GREEN}),
        dbc.NavLink("Geospatial Analysis", href="#geospatial", style={'color': MOSS_GREEN}),
        dbc.NavLink("Data Quality", href="#data-quality", style={'color': MOSS_GREEN}),
    ], vertical=True, pills=True),
], style={
    'position': 'fixed', 'top': 0, 'left': 0, 'bottom': 0, 'width': '250px', 'padding': '20px',
    'backgroundColor': MINE_SHAFT
})

# KPI Cards
kpi_cards = html.Div(id='kpi-cards', style={'margin': '20px', 'display': 'flex', 'justifyContent': 'space-around'})

# Download Button
download_button = html.A(
    dbc.Button("Download Detailed Report", id="download-btn", style={'backgroundColor': CHATEAU_GREEN, 'borderColor': CHATEAU_GREEN}),
    id="download-link", download="air_quality_report.pdf", href="", target="_blank",
    style={'margin': '10px'}
)

# Offline Warning
offline_warning = html.Div([
    html.P(
        "⚠️ Offline Mode Not Supported: Google Colab requires an internet connection. Download your notebook (.ipynb) to work offline and sync changes later.",
        style={'backgroundColor': '#ffeb3b', 'padding': '10px', 'textAlign': 'center', 'margin': '10px'}
    ),
], style={'display': 'block'})

# Privacy Warning
privacy_warning = html.Div([
    html.P(
        "🔒 Privacy Notice: Notebooks are stored on Google Drive. For sensitive data, download your notebook and store it locally.",
        style={'backgroundColor': '#f44336', 'color': '#fff', 'padding': '10px', 'textAlign': 'center', 'margin': '10px'}
    ),
], style={'display': 'block'})

# Collaboration Notification
collab_notification = html.Div([
    html.P(
        "👥 Multiple users are editing this notebook. Sync changes to avoid conflicts.",
        style={'backgroundColor': '#2196f3', 'color': '#fff', 'padding': '5px', 'margin': '10px', 'display': 'none'}
    ),
], id='collab-notification', style={'textAlign': 'center'})

# Runtime Warning and Auto-Save
runtime_status = html.Div([
    html.P("⏰ Warning: Session will disconnect after 12 hours. Save your work!",
           style={'color': '#f44336', 'margin': '10px'}),
    dbc.Checkbox(id='auto-save', label="Auto-Save", value=False, style={'margin': '10px'}),
], style={'backgroundColor': GRAY_NURSE, 'padding': '10px', 'margin': '10px'})

# Help Modal
help_modal = dbc.Modal([
    dbc.ModalHeader("Welcome to Air Quality Dashboard"),
    dbc.ModalBody([
        html.P("Here’s a quick guide to get started:"),
        html.Ul([
            html.Li("Run Code: Use the play button next to a code cell."),
            html.Li("Add Cells: Use the 'Insert' menu."),
            html.Li("Save Work: Click 'File > Save' or enable Auto-Save."),
            html.Li("Select Devices: Use the dropdown to filter data."),
            html.Li("Generate Report: Download the PDF report."),
        ]),
    ]),
    dbc.ModalFooter(dbc.Button("Close", id="close-help", className="ml-auto")),
], id="help-modal", is_open=False)

# Main Content
content = html.Div([
    html.H1("Air Quality Monitoring Dashboard", style={'textAlign': 'center', 'color': GRAY_NURSE, 'padding': '20px'}),
    offline_warning,
    privacy_warning,
    collab_notification,
    runtime_status,
    dbc.Button("Help", id="help-button", color="info", style={'margin': '10px'}),
    help_modal,
    kpi_cards,
    download_button,
    # Overview Section
    html.H2("Overview", id="overview", style={'textAlign': 'center', 'color': GRAY_NURSE}),
    html.Div([
        html.Div([
            dcc.Dropdown(id='overview-graph-type', options=[
                {'label': gc['graph_type'], 'value': gc['graph_type']} for gc in visualization_configs
            ], value='line', style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
            dcc.Dropdown(id='overview-x-axis', options=[
                {'label': col, 'value': col} for col in ['Datetime', 'Hour']
            ], value='Datetime', style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
            dcc.Dropdown(id='overview-y-axis', options=[
                {'label': col, 'value': col} for col in df.select_dtypes(include=[np.number]).columns
            ], value='PM2.5' if 'PM2.5' in df.columns else df.select_dtypes(include=[np.number]).columns[0],
                         style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
        ], style={'display': 'flex', 'justifyContent': 'space-between', 'display': 'none'}, id='advanced-overview-controls'),
        dcc.Graph(id='overview-panel'),
        html.Div(id='overview-insights', style={
            'marginTop': '10px', 'fontSize': '14px', 'color': MINE_SHAFT,
            'backgroundColor': GRAY_NURSE, 'padding': '10px', 'borderRadius': '5px'
        })
    ], style={'marginBottom': '40px'}),
    # Detailed Analysis Section
    html.H2("Detailed Analysis", id="analysis", style={'textAlign': 'center', 'color': GRAY_NURSE}),
    html.Div([
        html.Div([
            dcc.Dropdown(id='analysis-graph-type-1', options=[
                {'label': gc['graph_type'], 'value': gc['graph_type']} for gc in visualization_configs
            ], value='scatter', style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
            dcc.Dropdown(id='analysis-x-axis-1', options=[
                {'label': col, 'value': col} for col in df.select_dtypes(include=[np.number]).columns
            ], value='Temperature' if 'Temperature' in df.columns else df.select_dtypes(include=[np.number]).columns[0],
                         style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
            dcc.Dropdown(id='analysis-y-axis-1', options=[
                {'label': col, 'value': col} for col in df.select_dtypes(include=[np.number]).columns
            ], value='Humidity' if 'Humidity' in df.columns else df.select_dtypes(include=[np.number]).columns[0],
                         style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
        ], style={'display': 'flex', 'justifyContent': 'space-between', 'display': 'none'}, id='advanced-analysis-controls'),
        dcc.Graph(id='analysis-panel-1'),
        html.Div(id='analysis-insights-1', style={
            'marginTop': '10px', 'fontSize': '14px', 'color': MINE_SHAFT,
            'backgroundColor': GRAY_NURSE, 'padding': '10px', 'borderRadius': '5px'
        })
    ], style={'marginBottom': '40px'}),
    # Geospatial Analysis Section
    html.H2("Geospatial Analysis", id="geospatial", style={'textAlign': 'center', 'color': GRAY_NURSE}),
    html.Div([
        html.Div([
            dcc.Dropdown(id='geospatial-variable', options=[
                {'label': col, 'value': col} for col in ['PM2.5', 'PM10', 'Temperature', 'Humidity']
                if col in df.columns
            ], value='PM2.5' if 'PM2.5' in df.columns else df.select_dtypes(include=[np.number]).columns[0],
                         style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
        ], style={'display': 'flex', 'justifyContent': 'space-between', 'display': 'none'}, id='advanced-geospatial-controls'),
        dcc.Graph(id='geospatial-panel'),
        html.Div(id='geospatial-insights', style={
            'marginTop': '10px', 'fontSize': '14px', 'color': MINE_SHAFT,
            'backgroundColor': GRAY_NURSE, 'padding': '10px', 'borderRadius': '5px'
        })
    ], style={'marginBottom': '40px'}),
    # Data Quality Section
    html.H2("Data Quality", id="data-quality", style={'textAlign': 'center', 'color': GRAY_NURSE}),
    html.Div([
        html.Div([
            dcc.Dropdown(id='data-quality-graph-type', options=[
                {'label': gc['graph_type'], 'value': gc['graph_type']} for gc in visualization_configs
            ], value='bar_missing', style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
            dcc.Dropdown(id='data-quality-x-axis', options=[
                {'label': col, 'value': col} for col in ['Datetime', 'Hour']
            ], value='Datetime', style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
            dcc.Dropdown(id='data-quality-y-axis', options=[
                {'label': col, 'value': col} for col in df.select_dtypes(include=[np.number]).columns
            ], value='PM2.5' if 'PM2.5' in df.columns else df.select_dtypes(include=[np.number]).columns[0],
                         style={'margin': '10px', 'borderColor': MOSS_GREEN, 'backgroundColor': MINE_SHAFT, 'color': 'black'}),
        ], style={'display': 'flex', 'justifyContent': 'space-between', 'display': 'none'}, id='advanced-data-quality-controls'),
        dcc.Graph(id='data-quality-panel'),
        html.Div(id='data-quality-insights', style={
            'marginTop': '10px', 'fontSize': '14px', 'color': MINE_SHAFT,
            'backgroundColor': GRAY_NURSE, 'padding': '10px', 'borderRadius': '5px'
        })
    ]),
    html.Div(id='dashboard-insights', style={
        'margin': '20px', 'padding': '10px', 'backgroundColor': GRAY_NURSE,
        'borderRadius': '5px', 'color': MINE_SHAFT
    })
], style={
    'marginLeft': '270px', 'padding': '20px', 'backgroundColor': MINE_SHAFT
})

# Set the app layout
app.layout = html.Div([sidebar, content])

"""#**Step 8: Define Graph Generation Function**"""

# Function to generate figures with robust error handling
def generate_figure(config, data, x_axis=None, y_axis=None):
    try:
        if data.empty:
            raise ValueError("Dataframe is empty")
        graph_type = config['graph_type']
        vars_tuple = config['vars'] if not x_axis and not y_axis else (x_axis, y_axis) if x_axis and y_axis else config['vars']

        # Validate that the required variables exist in the DataFrame
        if graph_type in ['gauge', 'histogram', 'bar']:
            if not vars_tuple or len(vars_tuple) < 1:
                raise ValueError(f"Graph type '{graph_type}' requires at least one variable, got {vars_tuple}")
            var = vars_tuple[0]
            if var not in data.columns:
                raise ValueError(f"Variable '{var}' not found in DataFrame columns: {data.columns}")
            if data[var].isna().all():
                raise ValueError(f"All values for '{var}' are NaN")

        elif graph_type in ['line', 'scatter']:
            if not vars_tuple or len(vars_tuple) < 2:
                raise ValueError(f"Graph type '{graph_type}' requires two variables (x, y), got {vars_tuple}")
            x_var, y_var = vars_tuple
            if x_var not in data.columns:
                raise ValueError(f"X variable '{x_var}' not found in DataFrame columns: {data.columns}")
            if y_var not in data.columns:
                raise ValueError(f"Y variable '{y_var}' not found in DataFrame columns: {data.columns}")
            if data[x_var].isna().all():
                raise ValueError(f"All values for '{x_var}' are NaN")
            if data[y_var].isna().all():
                raise ValueError(f"All values for '{y_var}' are NaN")

        elif graph_type == 'scatter_map':
            if not vars_tuple or len(vars_tuple) < 2:
                raise ValueError(f"Graph type '{graph_type}' requires two variables (variable, Geohash), got {vars_tuple}")
            var, geohash_var = vars_tuple
            if var not in data.columns:
                raise ValueError(f"Variable '{var}' not found in DataFrame columns: {data.columns}")
            if geohash_var not in data.columns:
                raise ValueError(f"Geohash variable '{geohash_var}' not found in DataFrame columns: {data.columns}")
            if data[var].isna().all():
                raise ValueError(f"All values for '{var}' are NaN")
            if data[geohash_var].isna().all():
                raise ValueError(f"All values for '{geohash_var}' are NaN")

        elif graph_type == 'bar_missing':
            if data.isna().sum().sum() == 0:
                raise ValueError("No missing data to plot")

        # Generate the figure based on graph type
        if graph_type == 'gauge':
            var = vars_tuple[0]
            latest_value = data.groupby('Device_ID')[var].last().mean() if not data[var].isna().all() else 0
            range_max = {'PM2.5': 500, 'PM10': 500, 'Temperature': 50, 'Humidity': 100}.get(var, 100)
            thresholds = {
                'PM2.5': [10, 25, 50, 100],
                'PM10': [20, 50, 100, 200],
                'Temperature': [15, 25, 35, 45],
                'Humidity': [30, 50, 70, 90]
            }
            steps = [
                {'range': [0, thresholds[var][0]], 'color': MOSS_GREEN},
                {'range': [thresholds[var][0], thresholds[var][1]], 'color': "lightgreen"},
                {'range': [thresholds[var][1], thresholds[var][2]], 'color': 'yellow'},
                {'range': [thresholds[var][2], thresholds[var][3]], 'color': 'orange'},
                {'range': [thresholds[var][3], range_max], 'color': 'red'}
            ]
            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=latest_value,
                title={'text': f"Average Current {var}"},
                gauge={
                    'axis': {'range': [0, range_max]},
                    'bar': {'color': CHATEAU_GREEN},
                    'steps': steps
                }
            ))
            fig.update_layout(template="plotly_dark")

        elif graph_type == 'line':
            x_var = x_axis if x_axis in ['Datetime', 'Hour'] else 'Datetime'
            y_var = y_axis if y_axis in data.select_dtypes(include=[np.number]).columns else data.select_dtypes(include=[np.number]).columns[0]
            if x_var == 'Datetime':
                last_24h = data[data['Datetime'] >= data['Datetime'].max() - pd.Timedelta(hours=24)]
                if last_24h.empty or last_24h[y_var].isna().all():
                    raise ValueError(f"No data for {y_var} in last 24 hours")
                fig = px.line(
                    last_24h, x=x_var, y=y_var, color='Device_ID',
                    title=f"{y_var} Trends by Device",
                    color_discrete_sequence=[MOSS_GREEN, CHATEAU_GREEN, GRAY_NURSE]
                )
            elif x_var == 'Hour':
                fig = px.line(
                    data, x='Hour', y=y_var, color='Device_ID',
                    title=f"Diurnal {y_var} Patterns",
                    color_discrete_sequence=[MOSS_GREEN, CHATEAU_GREEN, GRAY_NURSE]
                )
            fig.update_layout(template="plotly_dark")

        elif graph_type == 'scatter':
            x_var = x_axis if x_axis in data.select_dtypes(include=[np.number]).columns else data.select_dtypes(include=[np.number]).columns[0]
            y_var = y_axis if y_axis in data.select_dtypes(include=[np.number]).columns else data.select_dtypes(include=[np.number]).columns[1] if len(data.select_dtypes(include=[np.number]).columns) > 1 else data.select_dtypes(include=[np.number]).columns[0]
            fig = px.scatter(
                data, x=x_var, y=y_var, color='Device_ID',
                title=f"{y_var} vs. {x_var}",
                color_discrete_sequence=[MOSS_GREEN, CHATEAU_GREEN, GRAY_NURSE]
            )
            fig.update_layout(template="plotly_dark")

        elif graph_type == 'histogram':
            var = y_axis if y_axis in data.select_dtypes(include=[np.number]).columns else data.select_dtypes(include=[np.number]).columns[0]
            fig = px.histogram(
                data, x=var, nbins=30, color='Device_ID',
                title=f"{var} Distribution",
                color_discrete_sequence=[MOSS_GREEN, CHATEAU_GREEN, GRAY_NURSE]
            )
            fig.update_layout(template="plotly_dark")

        elif graph_type == 'bar':
            var = vars_tuple[0]
            avg_by_device = data.groupby('Device_ID')[var].mean().reset_index()
            if avg_by_device.empty:
                raise ValueError(f"No data for {var} by Device_ID")
            fig = px.bar(
                avg_by_device, x='Device_ID', y=var,
                title=f"Average {var} by Device",
                color_discrete_sequence=[MOSS_GREEN]
            )
            fig.update_layout(template="plotly_dark")

        elif graph_type == 'bar_missing':
            missing_data = data.isna().mean() * 100
            missing_df = pd.DataFrame({'Column': missing_data.index, 'Missing Percentage': missing_data.values})
            fig = px.bar(
                missing_df, x='Missing Percentage', y='Column', orientation='h',
                title="Missing Data Summary (%)",
                color_discrete_sequence=[MOSS_GREEN]
            )
            fig.update_layout(template="plotly_dark")

        elif graph_type == 'scatter_map':
            var, geohash_var = vars_tuple
            # Decode Geohash to latitude and longitude
            data['lat_lon'] = data[geohash_var].apply(lambda x: geohash2.decode(x) if pd.notna(x) else (None, None))
            data['lat'] = data['lat_lon'].apply(lambda x: float(x[0]) if x else None)
            data['lon'] = data['lat_lon'].apply(lambda x: float(x[1]) if x else None)
            # Drop rows with invalid lat/lon
            plot_data = data.dropna(subset=['lat', 'lon'])
            if plot_data.empty:
                raise ValueError("No valid latitude/longitude data after decoding Geohash")
            fig = px.scatter_mapbox(
                plot_data,
                lat='lat',
                lon='lon',
                color=var,
                size=var,
                hover_data=['Device_ID', geohash_var],
                title=f"{var} by Location",
                zoom=10,
                mapbox_style="open-street-map",
                color_continuous_scale=px.colors.sequential.Viridis
            )
            fig.update_layout(template="plotly_dark")

        else:
            fig = go.Figure()
            fig.update_layout(
                template="plotly_dark",
                annotations=[dict(text=f"Unsupported graph type: {graph_type}", xref="paper", yref="paper", showarrow=False)]
            )
        return fig

    except Exception as e:
        logger.error(f"Error generating figure: {e}", extra={'flush': True})
        return go.Figure().update_layout(
            template="plotly_dark",
            annotations=[dict(text=f"Error generating figure: {e}", xref="paper", yref="paper", showarrow=False)]
        )

"""#**Step 9: Define Dashboard Callback and Run the App**"""

import base64
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import dash
from dash.dependencies import Input, Output, State
# import dash_html_components as html
from dash import html # Import html from dash directly
import dash_bootstrap_components as dbc
import logging

# Set up logging (consistent with previous steps)
logging.basicConfig(level=logging.INFO, force=True)
logger = logging.getLogger(__name__)

# Define color constants (from Step 7)
GRAY_NURSE = '#E8ECEF'  # Light gray for text and backgrounds
MOSS_GREEN = '#A3BFFA'  # Green for borders and links
MINE_SHAFT = '#2D2D2D'  # Dark gray for backgrounds
CHATEAU_GREEN = '#39A78E'  # Green for buttons

"""#**Step 9: Callback to Update the Dashboard**"""

@app.callback(
    [Output('overview-panel', 'figure'),
     Output('overview-insights', 'children'),
     Output('analysis-panel-1', 'figure'),
     Output('analysis-insights-1', 'children'),
     Output('geospatial-panel', 'figure'),
     Output('geospatial-insights', 'children'),
     Output('data-quality-panel', 'figure'),
     Output('data-quality-insights', 'children'),
     Output('kpi-cards', 'children'),
     Output('download-link', 'href'),
     Output('dashboard-insights', 'children'),
     Output('advanced-overview-controls', 'style'),
     Output('advanced-analysis-controls', 'style'),
     Output('advanced-geospatial-controls', 'style'),
     Output('advanced-data-quality-controls', 'style'),
     Output('collab-notification', 'style'),
     Output('help-modal', 'is_open')],
    [Input('device-dropdown', 'value'),
     Input('time-range', 'value'),
     Input('date-picker', 'start_date'),
     Input('date-picker', 'end_date'),
     Input('overview-graph-type', 'value'),
     Input('overview-x-axis', 'value'),
     Input('overview-y-axis', 'value'),
     Input('analysis-graph-type-1', 'value'),
     Input('analysis-x-axis-1', 'value'),
     Input('analysis-y-axis-1', 'value'),
     Input('geospatial-variable', 'value'),
     Input('data-quality-graph-type', 'value'),
     Input('data-quality-x-axis', 'value'),
     Input('data-quality-y-axis', 'value'),
     Input('mode-toggle', 'n_clicks'),
     Input('help-button', 'n_clicks'),
     Input('auto-save', 'value')],
    [State('help-modal', 'is_open')]
)
def update_dashboard(selected_devices, time_range, start_date, end_date,
                     overview_graph_type, overview_x_axis, overview_y_axis,
                     analysis_graph_type_1, analysis_x_axis_1, analysis_y_axis_1,
                     geospatial_variable,
                     data_quality_graph_type, data_quality_x_axis, data_quality_y_axis,
                     mode_clicks, help_clicks, auto_save, help_open):
    try:
        logger.info("Starting dashboard update", extra={'flush': True})
        if not selected_devices:
            selected_devices = df['Device_ID'].unique().tolist()

        filtered_df = df[df['Device_ID'].isin(selected_devices)].copy()
        if start_date and end_date:
            filtered_df = filtered_df[(filtered_df['Datetime'] >= pd.to_datetime(start_date)) & (filtered_df['Datetime'] <= pd.to_datetime(end_date))]
        if time_range == '24h':
            filtered_df = filtered_df[filtered_df['Datetime'] >= filtered_df['Datetime'].max() - pd.Timedelta(hours=24)]
        elif time_range == '48h':
            filtered_df = filtered_df[filtered_df['Datetime'] >= filtered_df['Datetime'].max() - pd.Timedelta(hours=48)]
        elif time_range == '7d':
            filtered_df = filtered_df[filtered_df['Datetime'] >= filtered_df['Datetime'].max() - pd.Timedelta(days=7)]
        logger.info(f"Filtered data shape: {filtered_df.shape}", extra={'flush': True})

        # Add validation for empty DataFrame
        if filtered_df.empty:
            logger.warning("Filtered DataFrame is empty after applying filters.", extra={'flush': True})
            empty_fig = go.Figure().update_layout(template="plotly_dark", annotations=[dict(text="No data available for the selected filters.", xref="paper", yref="paper", showarrow=False)])
            empty_output = html.Div([html.P("No data available for the selected filters.")])
            return (empty_fig, empty_output,
                    empty_fig, empty_output,
                    empty_fig, empty_output,
                    empty_fig, empty_output,
                    [html.P("No data available for KPIs")],
                    "",
                    html.Div([html.P("No data available.")]),
                    {'display': 'flex', 'justifyContent': 'space-between'},
                    {'display': 'flex', 'justifyContent': 'space-between'},
                    {'display': 'flex', 'justifyContent': 'space-between'},
                    {'display': 'flex', 'justifyContent': 'space-between'},
                    {'display': 'none'}, False)

        kpis = compute_kpis(filtered_df)
        kpi_cards_content = [
            dbc.Card([
                dbc.CardBody([
                    html.H4(title, style={'color': MINE_SHAFT}),
                    html.P(
                        f"{value:.2f}" + ("%" if "Rate" in title else "") if isinstance(value, (int, float)) else str(value),
                        style={'fontSize': '24px', 'color': MINE_SHAFT}
                    )
                ])
            ], style={'width': '18rem', 'backgroundColor': GRAY_NURSE, 'margin': '10px'})
            for title, value in kpis.items()
        ]

        def get_config(graph_type, x_axis, y_axis):
            # Default variables if user selection is invalid
            default_x = 'Datetime' if graph_type == 'line' else 'Temperature'
            default_y = 'PM2.5' if 'PM2.5' in df.columns else df.select_dtypes(include=[np.number]).columns[0]

            # Use user-selected x_axis and y_axis if valid, otherwise fall back to defaults
            x_axis = x_axis if x_axis in df.columns else default_x
            y_axis = y_axis if y_axis in df.columns and pd.api.types.is_numeric_dtype(df[y_axis]) else default_y

            for config in visualization_configs:
                if config['graph_type'] == graph_type:
                    if graph_type in ['gauge', 'histogram', 'bar', 'bar_missing']:
                        return {'vars': (y_axis,) if y_axis else config['vars'], 'graph_type': graph_type}
                    elif graph_type in ['line', 'scatter', 'scatter_map']:
                        return {'vars': (x_axis, y_axis) if x_axis and y_axis else config['vars'], 'graph_type': graph_type}
                    return config
            # Fallback configuration if graph type is not found
            return {'vars': (x_axis, y_axis) if graph_type in ['line', 'scatter', 'scatter_map'] else (y_axis,), 'graph_type': graph_type}

        # Generate figures and insights for each panel
        insights_dict = {}

        # Overview Panel
        try:
            overview_config = get_config(overview_graph_type, overview_x_axis, overview_y_axis)
            overview_fig = generate_figure(overview_config, filtered_df, overview_x_axis, overview_y_axis)
            overview_insights, overview_summary = generate_dynamic_insights(overview_config, filtered_df, selected_devices)
            overview_output = html.Div([
                html.Ul([html.Li(insight) for insight in overview_insights]),
                html.P(f"Summary: {overview_summary}", style={'fontWeight': 'bold'})
            ])
            insights_dict['overview'] = overview_insights
        except Exception as e:
            logger.error(f"Error in Overview panel: {e}", extra={'flush': True})
            overview_fig = go.Figure().update_layout(template="plotly_dark", annotations=[dict(text=f"Error in Overview panel: {e}", xref="paper", yref="paper", showarrow=False)])
            overview_insights = [f"Error generating insights: {e}"]
            overview_output = html.Div([
                html.Ul([html.Li(insight) for insight in overview_insights]),
                html.P("Summary: Error in Overview panel", style={'fontWeight': 'bold'})
            ])
            insights_dict['overview'] = overview_insights

        # Analysis Panel
        try:
            analysis_config_1 = get_config(analysis_graph_type_1, analysis_x_axis_1, analysis_y_axis_1)
            analysis_fig_1 = generate_figure(analysis_config_1, filtered_df, analysis_x_axis_1, analysis_y_axis_1)
            analysis_insights_1, analysis_summary_1 = generate_dynamic_insights(analysis_config_1, filtered_df, selected_devices)
            analysis_output_1 = html.Div([
                html.Ul([html.Li(insight) for insight in analysis_insights_1]),
                html.P(f"Summary: {analysis_summary_1}", style={'fontWeight': 'bold'})
            ])
            insights_dict['analysis'] = analysis_insights_1
        except Exception as e:
            logger.error(f"Error in Analysis panel: {e}", extra={'flush': True})
            analysis_fig_1 = go.Figure().update_layout(template="plotly_dark", annotations=[dict(text=f"Error in Analysis panel: {e}", xref="paper", yref="paper", showarrow=False)])
            analysis_insights_1 = [f"Error generating insights: {e}"]
            analysis_output_1 = html.Div([
                html.Ul([html.Li(insight) for insight in analysis_insights_1]),
                html.P("Summary: Error in Analysis panel", style={'fontWeight': 'bold'})
            ])
            insights_dict['analysis'] = analysis_insights_1

        # Geospatial Panel
        try:
            geospatial_config = {'vars': (geospatial_variable, 'Geohash'), 'graph_type': 'scatter_map'}
            geospatial_fig = generate_figure(geospatial_config, filtered_df)
            geospatial_insights, geospatial_summary = generate_dynamic_insights(geospatial_config, filtered_df, selected_devices)
            geospatial_output = html.Div([
                html.Ul([html.Li(insight) for insight in geospatial_insights]),
                html.P(f"Summary: {geospatial_summary}", style={'fontWeight': 'bold'})
            ])
            insights_dict['geospatial'] = geospatial_insights
        except Exception as e:
            logger.error(f"Error in Geospatial panel: {e}", extra={'flush': True})
            geospatial_fig = go.Figure().update_layout(template="plotly_dark", annotations=[dict(text=f"Error in Geospatial panel: {e}", xref="paper", yref="paper", showarrow=False)])
            geospatial_insights = [f"Error generating insights: {e}"]
            geospatial_output = html.Div([
                html.Ul([html.Li(insight) for insight in geospatial_insights]),
                html.P("Summary: Error in Geospatial panel", style={'fontWeight': 'bold'})
            ])
            insights_dict['geospatial'] = geospatial_insights

        # Data Quality Panel
        try:
            data_quality_config = get_config(data_quality_graph_type, data_quality_x_axis, data_quality_y_axis)
            data_quality_fig = generate_figure(data_quality_config, filtered_df, data_quality_x_axis, data_quality_y_axis)
            data_quality_insights, data_quality_summary = generate_dynamic_insights(data_quality_config, filtered_df, selected_devices)
            data_quality_output = html.Div([
                html.Ul([html.Li(insight) for insight in data_quality_insights]),
                html.P(f"Summary: {data_quality_summary}", style={'fontWeight': 'bold'})
            ])
            insights_dict['data_quality'] = data_quality_insights
        except Exception as e:
            logger.error(f"Error in Data Quality panel: {e}", extra={'flush': True})
            data_quality_fig = go.Figure().update_layout(template="plotly_dark", annotations=[dict(text=f"Error in Data Quality panel: {e}", xref="paper", yref="paper", showarrow=False)])
            data_quality_insights = [f"Error generating insights: {e}"]
            data_quality_output = html.Div([
                html.Ul([html.Li(insight) for insight in data_quality_insights]),
                html.P("Summary: Error in Data Quality panel", style={'fontWeight': 'bold'})
            ])
            insights_dict['data_quality'] = data_quality_insights

        # Dashboard Insights
        all_insights = overview_insights + analysis_insights_1 + geospatial_insights + data_quality_insights
        dashboard_output = html.Div([
            html.H3("Dashboard Insights", style={'color': MINE_SHAFT}),
            html.Ul([html.Li(insight) for insight in all_insights]),
            html.P("Summary of air quality data.", style={'fontWeight': 'bold'})
        ])

        # PDF Report
        try:
            pdf = generate_pdf_report(overview_fig, analysis_fig_1, data_quality_fig, geospatial_fig, insights_dict, kpis, filtered_df)
            if not pdf:
                logger.error("PDF generation returned empty bytes", extra={'flush': True})
                raise ValueError("PDF generation failed, returned 0 bytes")
            encoded_pdf = base64.b64encode(pdf).decode()
            download_href = f"data:application/pdf;base64,{encoded_pdf}"
        except Exception as e:
            logger.error(f"Error generating PDF: {e}", extra={'flush': True})
            download_href = ""

        # Beginner Mode Logic
        mode_style = {'display': 'flex', 'justifyContent': 'space-between'}  # Default to showing controls
        if mode_clicks and mode_clicks % 2 == 1:  # Toggle on odd clicks
            mode_style['display'] = 'none'

        # Collaboration Notification (Disabled for now)
        collab_style = {'display': 'none'}  # Placeholder for real implementation

        # Help Modal Logic
        help_open_next = not help_open if help_clicks else help_open

        logger.info("Dashboard update completed successfully", extra={'flush': True})
        return (overview_fig, overview_output,
                analysis_fig_1, analysis_output_1,
                geospatial_fig, geospatial_output,
                data_quality_fig, data_quality_output,
                kpi_cards_content,
                download_href,
                dashboard_output,
                mode_style, mode_style, mode_style, mode_style,
                collab_style, help_open_next)

    except Exception as e:
        logger.error(f"Callback error: {e}", extra={'flush': True})
        empty_fig = go.Figure().update_layout(template="plotly_dark", annotations=[dict(text=f"Error: {e}", xref="paper", yref="paper", showarrow=False)])
        empty_output = html.Div([html.P(f"Error: {e}")])
        return (empty_fig, empty_output,
                empty_fig, empty_output,
                empty_fig, empty_output,
                empty_fig, empty_output,
                [html.P("Error in KPIs")],
                "",
                html.Div([html.P(f"Error in dashboard: {e}")]),
                {'display': 'flex', 'justifyContent': 'space-between'},
                {'display': 'flex', 'justifyContent': 'space-between'},
                {'display': 'flex', 'justifyContent': 'space-between'},
                {'display': 'flex', 'justifyContent': 'space-between'},
                {'display': 'none'}, False)

# Run the Dash app
if __name__ == '__main__':
    app.run(debug=True)

